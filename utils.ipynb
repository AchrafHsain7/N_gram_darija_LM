{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_frequency(data, pattern):\n",
    "    total_freq = 0\n",
    "    for sentence in data:\n",
    "        total_freq += \"\".join(sentence).count(pattern)\n",
    "        # print(\"\".join(sentence).count(pattern))\n",
    "    \n",
    "    return total_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_higher_pattern(data, vocab, debug=False):\n",
    "    max_frq = 0\n",
    "    highest_tkn = \"\"\n",
    "    for i in vocab:\n",
    "        for j in vocab:\n",
    "            if i == \"\" or j ==\"\":\n",
    "                continue\n",
    "\n",
    "            frq = pattern_frequency(data, i+j)\n",
    "            if frq > max_frq:\n",
    "                max_frq = frq\n",
    "                highest_tkn = i + j\n",
    "                old_tokens = (i, j)\n",
    "                if debug:\n",
    "                    print(highest_tkn)\n",
    "                    print(old_tokens)\n",
    "                    print(\"=======\")\n",
    "    if max_frq == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    return  highest_tkn, old_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(data, vocab, k=25, debug=False):\n",
    "        for i in range(k):\n",
    "            print(i, end=\"-\")\n",
    "            new_token, old_tokens = current_higher_pattern(data, vocab, debug=debug)\n",
    "            if new_token == 0 and old_tokens == 0:\n",
    "                 return data, vocab\n",
    "            \n",
    "            vocab.append(new_token)\n",
    "            print(old_tokens)\n",
    "            vocab.remove(old_tokens[0])\n",
    "            if old_tokens[0] != old_tokens[1]:\n",
    "                vocab.remove(old_tokens[1])\n",
    "\n",
    "            # for j in range(len(data)):\n",
    "                # print(old_tokens[0])\n",
    "                # print(old_tokens[1])\n",
    "                # print(new_token)\n",
    "                # print(\"=======\")\n",
    "                # for tokn in range(len(data[j]) - 1):\n",
    "                #     print(data[j][tokn] + data[j][tokn+1])\n",
    "                #     print(old_tokens[0] + old_tokens[1])\n",
    "                #     print(\"=========\")\n",
    "                # data[j] = [new_token if (data[j][tokn] + data[j][tokn+1] == old_tokens[0] + old_tokens[1]) else data[j][token] for token in range(len(data[j]) -1)]\n",
    "\n",
    "        \n",
    "        \n",
    "        return data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bpe(data, vocab):\n",
    "    for i, tkn in enumerate(vocab):\n",
    "        print(i)\n",
    "        for sent in data:\n",
    "            sent_str = \"\".join(sent)\n",
    "            while True:\n",
    "                x = sent_str.find(tkn)\n",
    "                if x == -1:\n",
    "                    break\n",
    "                del sent[x: x + len(tkn)]\n",
    "                sent.insert(x, tkn)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus_chars.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = f.readlines()\n",
    "        vocab = [x.replace(\"\\n\", \"\") for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'ﺔ', 'ﺷ', 'Ç', 'ﻦ', '7', '👊', '👑', '😔', '🔪', '🏻', '٩', '💪', '…', '⛅', '🌾', 'J', '✋', 'ü', '🌨', '💖', 'ﺤ', '٥', '♬', '🌓', '🌲', '(', '😲', '•', '’', 'ﻷ', '😋', '👍', '😙', '💠', '🤭', 'ُ', '√', 'f', 'ﺴ', '○', '🙆', '😶', '😩', 'n', 'ﻙ', '🙄', '😢', '🚒', '💦', '♤', '👄', '8', '🐓', '💬', 'r', '💘', '😴', 'E', 'ذ', '✅', '🛡', '🔚', 'ن', '.', '\\u2069', '☆', ')', '💏', '🍃', '😅', 'ی', '🏃', 'ٔ', '👈', 'ﻳ', 'ر', '🥺', '🍫', '😠', 'è', '°', '›', 'N', '️', 'O', 'م', 'ک', 'ڭ', 'ل', '📧', '🪵', '🔱', '9', '✴', 'm', ',', '📱', '📞', 'I', '🖤', 'P', '💫', '🇦', 'ٓ', '😵', 'ㅤ', '%', 'ﺸ', '😂', '😃', '✊', 'U', 'j', '💃', '⬇', 'أ', '،', '⭐', 'س', 'w', 'ﺣ', '🌿', 'ﻤ', '😄', '😆', '🗡', 'ا', '🚑', '😞', '💜', '🚨', '🤍', 'إ', 'َ', 'ﺧ', 'Ö', '🇲', 'y', 'ز', 'd', 'ı', 's', '🔜', 'ٱ', '🌈', 'x', '\\\\', '=', '🌌', '👨', 'ﺡ', 'F', 'ş', '😕', 'Z', '؛', '\\xad', '!', '💯', '}', '🎶', 'Q', '⊙', '👶', 'D', '🍆', 'ﻞ', '😌', '1', '—', 'ò', '#', '٤', 'ﻹ', 'چ', '💣', 'ú', 'آ', '{', '💍', '🌚', '💔', '😤', 'ص', '😦', '‼', 'ك', '😐', 'غ', '|', '🕳', '😚', '🇺', '😰', '😹', 'ﺻ', '¿', 'î', '🐸', 'ْ', 'ﻌ', '¥', '»', 'ځ', '💢', 'ِ', '🔮', '🌟', 'G', 'ﻲ', '٣', '🕗', '💧', 'Y', 'ﻬ', '🎵', 'ﻨ', 'C', '🎈', '▽', '😗', 'ق', 'ﻧ', '🌺', '‹', '♦', '🔰', '\\u2066', '🙊', '👮', 'ﺳ', '🌐', 'ع', '🤔', 'گ', '*', 'ض', '🔥', '٢', '😭', '🐍', '📃', 'ٰ', '😨', '؟', '🍂', 'ﺏ', 'ب', '„', '\\u202c', 'p', '🍑', '🚗', '😏', '👏', '0', 'ؤ', 'ﻴ', 'ﺼ', 'ﺃ', '👂', '😉', '♀', '♧', '😊', '🤕', '?', 'ﻀ', '😑', 'ﻭ', 'l', '🎲', '🌗', '👺', '🌞', 'ہ', 'g', '❗', '→', '🙀', 'ث', '٦', '💨', '▼', '🔈', '💋', '🔙', '💥', 'ﺖ', '😘', '×', '[', '🔔', 'ً', '❌', 'â', '☝', '😳', \"'\", 'ê', 'é', '😈', 'ñ', '■', 'ۣ', '«', '🌬', 'ٌ', 'ﻑ', '😛', 'ﺪ', '👌', '🙌', '💕', 'ڄ', '💚', 'ح', '🗯', '⛔', '🙂', '@', 'ي', '✍', 'ﻰ', '🛢', '-', '🎬', '😇', '⚪', '♨', 'ﻮ', '😟', '😿', '🍀', '🙅', 'T', '😓', 'V', '💉', '》', '_', 'ۅ', 'ّ', '👐', '👇', '‘', 'ﺘ', '🏼', '💌', '⚜', 'ﺑ', '♣', '《', '£', '▪', '\"', 'B', '٠', '٪', 'k', 'ﻝ', '●', 'ف', 'ﻵ', '🎉', 'ڤ', '”', '📍', 'ö', 'ﺕ', 'K', '⚡', 'ﻟ', '🕯', '🤤', 'ﻛ', 'ﻩ', 'h', '١', '⏱', 'ﺄ', '🌹', 'o', '3', '\\u202b', 'ﻣ', 'ﺎ', 'v', 'ﻓ', '♥', 'ï', '🌳', '💞', '❇', '🥕', 'ى', '💑', '2', 'ت', '🚫', 'ة', '🧿', '✨', '❤', 't', 'ظ', '💀', 'ﻥ', ' ', '¤', 'a', '>', '😍', 'X', '“', 'خ', '&', '\\u061c', 'ﺠ', '⏳', '☪', 'و', '5', '\\u200d', '😥', '💓', 'ﺍ', 'ش', 'ﻢ', '🌧', 'R', 'ڨ', 'ä', 'b', '﴾', 'ó', 'ه', '💎', '٨', '😣', '🤫', 'ٕ', '🌷', '☂', 'ﻋ', '✳', 'ج', '🇸', ':', '🌀', '<', 'ݣ', '🌸', '😱', 'z', '6', 'ۗ', 'ﺀ', 'ۚ', '﴿', '🍁', 'i', ']', 'c', '▶', '😯', '🔞', '😜', '☹', 'ﺮ', 'ڥ', 'A', 'ﺺ', 'Á', '😮', '🤨', '🤗', 'ئ', 'ﻘ', '❣', 'í', '🔶', '😒', '◇', '😪', 'W', '⏩', 'ﺩ', '^', '👉', '☀', '〽', 'ﻚ', 'á', 'ﻠ', 'ﻈ', 'د', 'q', 'ﺟ', 'ٍ', 'e', '4', ';', '♡', 'ğ', 'ﻻ', 'ç', '😡', 'u', '🤣', 'ﺒ', '🕕', 'ط', '😁', 'ﺢ', '🗨', '🏚', '+', 'L', '٧', '❄', '💟', 'M', 'ډ', 'ـ', '☺', '💝', '😷', 'ﻔ', '/', 'ء', 'S', 'پ', '~', '🌤', 'ﺭ', '😧']\n",
      "538\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ر', 'ؤ', 'ي', 'ا', ' ', 'م', 'س', 'ح', 'ت', ' ', 'ع', 'ي', 'ن', 'ي', 'ه', 'ا', ' ', 'و', 'ع', 'ل', 'ا', 'ت', ' ', 'ع', 'ي', 'ن', 'ي', 'ه', 'ا', ' ', 'ف', 'ي', 'ه', ' ', 'و', 'ك', 'ل', 'ش', 'ي', ' ', 'ل', 'ي', ' ', 'ك', 'ا', 'ن', 'و', ' ', 'ح', 'ا', 'ط', 'ي', 'ن', ' ', 'ل', 'ل', 'ع', 'ي', 'ن', ' ', 'ع', 'ل', 'ي', 'ه', 'ا', '.', '.', '.', 'ش', 'ح', 'ا', 'ل', ' ', 'ب', 'ا', 'ش', ' ', 'ع', 'ا', 'د', ' ', 'ه', 'ض', 'ر', 'ت']]\n"
     ]
    }
   ],
   "source": [
    "test_text = ['رؤيا مسحت عينيها وعلات عينيها فيه وكلشي لي كانو حاطين للعين عليها...شحال باش عاد هضرت']\n",
    "data = [[*x] for x in test_text]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-('ي', 'ن')\n",
      "1-(' ', 'ع')\n",
      "2-('ه', 'ا')\n",
      "3-('ت', ' ع')\n",
      "4-('ت ع', 'ين')\n",
      "5-('.', '.')\n",
      "6-('ر', 'ؤ')\n",
      "7-('م', 'س')\n",
      "8-('ل', 'ل')\n",
      "9-('ح', 'ت عين')\n",
      "10-('و', 'ك')\n",
      "11-('ها', '..')\n",
      "12-('مس', 'حت عين')\n",
      "13-"
     ]
    }
   ],
   "source": [
    "tokenized_data, vocab_bpe = bpe(data, vocab.copy(), k=30, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'ﺔ', 'ﺷ', 'Ç', 'ﻦ', '7', '👊', '👑', '😔', '🔪', '🏻', '٩', '💪', '…', '⛅', '🌾', 'J', '✋', 'ü', '🌨', '💖', 'ﺤ', '٥', '♬', '🌓', '🌲', '(', '😲', '•', '’', 'ﻷ', '😋', '👍', '😙', '💠', '🤭', 'ُ', '√', 'f', 'ﺴ', '○', '🙆', '😶', '😩', 'n', 'ﻙ', '🙄', '😢', '🚒', '💦', '♤', '👄', '8', '🐓', '💬', 'r', '💘', '😴', 'E', 'ذ', '✅', '🛡', '🔚', '\\u2069', '☆', ')', '💏', '🍃', '😅', 'ی', '🏃', 'ٔ', '👈', 'ﻳ', '🥺', '🍫', '😠', 'è', '°', '›', 'N', '️', 'O', 'ک', 'ڭ', '📧', '🪵', '🔱', '9', '✴', 'm', ',', '📱', '📞', 'I', '🖤', 'P', '💫', '🇦', 'ٓ', '😵', 'ㅤ', '%', 'ﺸ', '😂', '😃', '✊', 'U', 'j', '💃', '⬇', 'أ', '،', '⭐', 'w', 'ﺣ', '🌿', 'ﻤ', '😄', '😆', '🗡', '🚑', '😞', '💜', '🚨', '🤍', 'إ', 'َ', 'ﺧ', 'Ö', '🇲', 'y', 'ز', 'd', 'ı', 's', '🔜', 'ٱ', '🌈', 'x', '\\\\', '=', '🌌', '👨', 'ﺡ', 'F', 'ş', '😕', 'Z', '؛', '\\xad', '!', '💯', '}', '🎶', 'Q', '⊙', '👶', 'D', '🍆', 'ﻞ', '😌', '1', '—', 'ò', '#', '٤', 'ﻹ', 'چ', '💣', 'ú', 'آ', '{', '💍', '🌚', '💔', '😤', 'ص', '😦', '‼', '😐', 'غ', '|', '🕳', '😚', '🇺', '😰', '😹', 'ﺻ', '¿', 'î', '🐸', 'ْ', 'ﻌ', '¥', '»', 'ځ', '💢', 'ِ', '🔮', '🌟', 'G', 'ﻲ', '٣', '🕗', '💧', 'Y', 'ﻬ', '🎵', 'ﻨ', 'C', '🎈', '▽', '😗', 'ق', 'ﻧ', '🌺', '‹', '♦', '🔰', '\\u2066', '🙊', '👮', 'ﺳ', '🌐', '🤔', 'گ', '*', 'ض', '🔥', '٢', '😭', '🐍', '📃', 'ٰ', '😨', '؟', '🍂', 'ﺏ', 'ب', '„', '\\u202c', 'p', '🍑', '🚗', '😏', '👏', '0', 'ﻴ', 'ﺼ', 'ﺃ', '👂', '😉', '♀', '♧', '😊', '🤕', '?', 'ﻀ', '😑', 'ﻭ', 'l', '🎲', '🌗', '👺', '🌞', 'ہ', 'g', '❗', '→', '🙀', 'ث', '٦', '💨', '▼', '🔈', '💋', '🔙', '💥', 'ﺖ', '😘', '×', '[', '🔔', 'ً', '❌', 'â', '☝', '😳', \"'\", 'ê', 'é', '😈', 'ñ', '■', 'ۣ', '«', '🌬', 'ٌ', 'ﻑ', '😛', 'ﺪ', '👌', '🙌', '💕', 'ڄ', '💚', '🗯', '⛔', '🙂', '@', '✍', 'ﻰ', '🛢', '-', '🎬', '😇', '⚪', '♨', 'ﻮ', '😟', '😿', '🍀', '🙅', 'T', '😓', 'V', '💉', '》', '_', 'ۅ', 'ّ', '👐', '👇', '‘', 'ﺘ', '🏼', '💌', '⚜', 'ﺑ', '♣', '《', '£', '▪', '\"', 'B', '٠', '٪', 'k', 'ﻝ', '●', 'ف', 'ﻵ', '🎉', 'ڤ', '”', '📍', 'ö', 'ﺕ', 'K', '⚡', 'ﻟ', '🕯', '🤤', 'ﻛ', 'ﻩ', 'h', '١', '⏱', 'ﺄ', '🌹', 'o', '3', '\\u202b', 'ﻣ', 'ﺎ', 'v', 'ﻓ', '♥', 'ï', '🌳', '💞', '❇', '🥕', 'ى', '💑', '2', '🚫', 'ة', '🧿', '✨', '❤', 't', 'ظ', '💀', 'ﻥ', '¤', 'a', '>', '😍', 'X', '“', 'خ', '&', '\\u061c', 'ﺠ', '⏳', '☪', '5', '\\u200d', '😥', '💓', 'ﺍ', 'ش', 'ﻢ', '🌧', 'R', 'ڨ', 'ä', 'b', '﴾', 'ó', '💎', '٨', '😣', '🤫', 'ٕ', '🌷', '☂', 'ﻋ', '✳', 'ج', '🇸', ':', '🌀', '<', 'ݣ', '🌸', '😱', 'z', '6', 'ۗ', 'ﺀ', 'ۚ', '﴿', '🍁', 'i', ']', 'c', '▶', '😯', '🔞', '😜', '☹', 'ﺮ', 'ڥ', 'A', 'ﺺ', 'Á', '😮', '🤨', '🤗', 'ئ', 'ﻘ', '❣', 'í', '🔶', '😒', '◇', '😪', 'W', '⏩', 'ﺩ', '^', '👉', '☀', '〽', 'ﻚ', 'á', 'ﻠ', 'ﻈ', 'د', 'q', 'ﺟ', 'ٍ', 'e', '4', ';', '♡', 'ğ', 'ﻻ', 'ç', '😡', 'u', '🤣', 'ﺒ', '🕕', 'ط', '😁', 'ﺢ', '🗨', '🏚', '+', 'L', '٧', '❄', '💟', 'M', 'ډ', 'ـ', '☺', '💝', '😷', 'ﻔ', '/', 'ء', 'S', 'پ', '~', '🌤', 'ﺭ', '😧', 'رؤ', 'لل', 'وك', 'ها..', 'مسحت عين']\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "print(vocab_bpe)\n",
    "print(len(vocab_bpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ر', 'ؤ', 'ي', 'ا', ' ', 'م', 'س', 'ح', 'ت', ' ', 'ع', 'ي', 'ن', 'ي', 'ه', 'ا', ' ', 'و', 'ع', 'ل', 'ا', 'ت', ' ', 'ع', 'ي', 'ن', 'ي', 'ه', 'ا', ' ', 'ف', 'ي', 'ه', ' ', 'و', 'ك', 'ل', 'ش', 'ي', ' ', 'ل', 'ي', ' ', 'ك', 'ا', 'ن', 'و', ' ', 'ح', 'ا', 'ط', 'ي', 'ن', ' ', 'ل', 'ل', 'ع', 'ي', 'ن', ' ', 'ع', 'ل', 'ي', 'ه', 'ا', '.', '.', '.', 'ش', 'ح', 'ا', 'ل', ' ', 'ب', 'ا', 'ش', ' ', 'ع', 'ا', 'د', ' ', 'ه', 'ض', 'ر', 'ت']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokenize_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_bpe\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtokenize_bpe\u001b[1;34m(data, vocab)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m sent[x: x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tkn)]\n\u001b[1;32m---> 11\u001b[0m             \u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtkn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenize_bpe(tokenized_data, vocab_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ض\n"
     ]
    }
   ],
   "source": [
    "print(vocab_bpe[228])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
