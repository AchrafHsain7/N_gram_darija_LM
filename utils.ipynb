{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_frequency(data, pattern):\n",
    "    total_freq = 0\n",
    "    for sentence in data:\n",
    "        total_freq += \"\".join(sentence).count(pattern)\n",
    "        # print(\"\".join(sentence).count(pattern))\n",
    "    \n",
    "    return total_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_higher_pattern(data, vocab, debug=False):\n",
    "    max_frq = 0\n",
    "    highest_tkn = \"\"\n",
    "    for i in vocab:\n",
    "        for j in vocab:\n",
    "            if i == \"\" or j ==\"\":\n",
    "                continue\n",
    "\n",
    "            frq = pattern_frequency(data, i+j)\n",
    "            if frq > max_frq:\n",
    "                max_frq = frq\n",
    "                highest_tkn = i + j\n",
    "                old_tokens = (i, j)\n",
    "                if debug:\n",
    "                    print(highest_tkn)\n",
    "                    print(old_tokens)\n",
    "                    print(\"=======\")\n",
    "    if max_frq == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    return  highest_tkn, old_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(data, vocab, k=25, debug=False):\n",
    "        for i in range(k):\n",
    "            print(i, end=\"-\")\n",
    "            new_token, old_tokens = current_higher_pattern(data, vocab, debug=debug)\n",
    "            if new_token == 0 and old_tokens == 0:\n",
    "                 return data, vocab\n",
    "            \n",
    "            vocab.append(new_token)\n",
    "            print(old_tokens)\n",
    "            vocab.remove(old_tokens[0])\n",
    "            if old_tokens[0] != old_tokens[1]:\n",
    "                vocab.remove(old_tokens[1])\n",
    "\n",
    "            # for j in range(len(data)):\n",
    "                # print(old_tokens[0])\n",
    "                # print(old_tokens[1])\n",
    "                # print(new_token)\n",
    "                # print(\"=======\")\n",
    "                # for tokn in range(len(data[j]) - 1):\n",
    "                #     print(data[j][tokn] + data[j][tokn+1])\n",
    "                #     print(old_tokens[0] + old_tokens[1])\n",
    "                #     print(\"=========\")\n",
    "                # data[j] = [new_token if (data[j][tokn] + data[j][tokn+1] == old_tokens[0] + old_tokens[1]) else data[j][token] for token in range(len(data[j]) -1)]\n",
    "\n",
    "        \n",
    "        \n",
    "        return data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bpe(data, vocab):\n",
    "    for i, tkn in enumerate(vocab):\n",
    "        print(i)\n",
    "        for sent in data:\n",
    "            sent_str = \"\".join(sent)\n",
    "            while True:\n",
    "                x = sent_str.find(tkn)\n",
    "                if x == -1:\n",
    "                    break\n",
    "                del sent[x: x + len(tkn)]\n",
    "                sent.insert(x, tkn)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus_chars.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = f.readlines()\n",
    "        vocab = [x.replace(\"\\n\", \"\") for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'ïº”', 'ïº·', 'Ã‡', 'ï»¦', '7', 'ğŸ‘Š', 'ğŸ‘‘', 'ğŸ˜”', 'ğŸ”ª', 'ğŸ»', 'Ù©', 'ğŸ’ª', 'â€¦', 'â›…', 'ğŸŒ¾', 'J', 'âœ‹', 'Ã¼', 'ğŸŒ¨', 'ğŸ’–', 'ïº¤', 'Ù¥', 'â™¬', 'ğŸŒ“', 'ğŸŒ²', '(', 'ğŸ˜²', 'â€¢', 'â€™', 'ï»·', 'ğŸ˜‹', 'ğŸ‘', 'ğŸ˜™', 'ğŸ’ ', 'ğŸ¤­', 'Ù', 'âˆš', 'f', 'ïº´', 'â—‹', 'ğŸ™†', 'ğŸ˜¶', 'ğŸ˜©', 'n', 'ï»™', 'ğŸ™„', 'ğŸ˜¢', 'ğŸš’', 'ğŸ’¦', 'â™¤', 'ğŸ‘„', '8', 'ğŸ“', 'ğŸ’¬', 'r', 'ğŸ’˜', 'ğŸ˜´', 'E', 'Ø°', 'âœ…', 'ğŸ›¡', 'ğŸ”š', 'Ù†', '.', '\\u2069', 'â˜†', ')', 'ğŸ’', 'ğŸƒ', 'ğŸ˜…', 'ÛŒ', 'ğŸƒ', 'Ù”', 'ğŸ‘ˆ', 'ï»³', 'Ø±', 'ğŸ¥º', 'ğŸ«', 'ğŸ˜ ', 'Ã¨', 'Â°', 'â€º', 'N', 'ï¸', 'O', 'Ù…', 'Ú©', 'Ú­', 'Ù„', 'ğŸ“§', 'ğŸªµ', 'ğŸ”±', '9', 'âœ´', 'm', ',', 'ğŸ“±', 'ğŸ“', 'I', 'ğŸ–¤', 'P', 'ğŸ’«', 'ğŸ‡¦', 'Ù“', 'ğŸ˜µ', 'ã…¤', '%', 'ïº¸', 'ğŸ˜‚', 'ğŸ˜ƒ', 'âœŠ', 'U', 'j', 'ğŸ’ƒ', 'â¬‡', 'Ø£', 'ØŒ', 'â­', 'Ø³', 'w', 'ïº£', 'ğŸŒ¿', 'ï»¤', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ—¡', 'Ø§', 'ğŸš‘', 'ğŸ˜', 'ğŸ’œ', 'ğŸš¨', 'ğŸ¤', 'Ø¥', 'Ù', 'ïº§', 'Ã–', 'ğŸ‡²', 'y', 'Ø²', 'd', 'Ä±', 's', 'ğŸ”œ', 'Ù±', 'ğŸŒˆ', 'x', '\\\\', '=', 'ğŸŒŒ', 'ğŸ‘¨', 'ïº¡', 'F', 'ÅŸ', 'ğŸ˜•', 'Z', 'Ø›', '\\xad', '!', 'ğŸ’¯', '}', 'ğŸ¶', 'Q', 'âŠ™', 'ğŸ‘¶', 'D', 'ğŸ†', 'ï»', 'ğŸ˜Œ', '1', 'â€”', 'Ã²', '#', 'Ù¤', 'ï»¹', 'Ú†', 'ğŸ’£', 'Ãº', 'Ø¢', '{', 'ğŸ’', 'ğŸŒš', 'ğŸ’”', 'ğŸ˜¤', 'Øµ', 'ğŸ˜¦', 'â€¼', 'Ùƒ', 'ğŸ˜', 'Øº', '|', 'ğŸ•³', 'ğŸ˜š', 'ğŸ‡º', 'ğŸ˜°', 'ğŸ˜¹', 'ïº»', 'Â¿', 'Ã®', 'ğŸ¸', 'Ù’', 'ï»Œ', 'Â¥', 'Â»', 'Ú', 'ğŸ’¢', 'Ù', 'ğŸ”®', 'ğŸŒŸ', 'G', 'ï»²', 'Ù£', 'ğŸ•—', 'ğŸ’§', 'Y', 'ï»¬', 'ğŸµ', 'ï»¨', 'C', 'ğŸˆ', 'â–½', 'ğŸ˜—', 'Ù‚', 'ï»§', 'ğŸŒº', 'â€¹', 'â™¦', 'ğŸ”°', '\\u2066', 'ğŸ™Š', 'ğŸ‘®', 'ïº³', 'ğŸŒ', 'Ø¹', 'ğŸ¤”', 'Ú¯', '*', 'Ø¶', 'ğŸ”¥', 'Ù¢', 'ğŸ˜­', 'ğŸ', 'ğŸ“ƒ', 'Ù°', 'ğŸ˜¨', 'ØŸ', 'ğŸ‚', 'ïº', 'Ø¨', 'â€', '\\u202c', 'p', 'ğŸ‘', 'ğŸš—', 'ğŸ˜', 'ğŸ‘', '0', 'Ø¤', 'ï»´', 'ïº¼', 'ïºƒ', 'ğŸ‘‚', 'ğŸ˜‰', 'â™€', 'â™§', 'ğŸ˜Š', 'ğŸ¤•', '?', 'ï»€', 'ğŸ˜‘', 'ï»­', 'l', 'ğŸ²', 'ğŸŒ—', 'ğŸ‘º', 'ğŸŒ', 'Û', 'g', 'â—', 'â†’', 'ğŸ™€', 'Ø«', 'Ù¦', 'ğŸ’¨', 'â–¼', 'ğŸ”ˆ', 'ğŸ’‹', 'ğŸ”™', 'ğŸ’¥', 'ïº–', 'ğŸ˜˜', 'Ã—', '[', 'ğŸ””', 'Ù‹', 'âŒ', 'Ã¢', 'â˜', 'ğŸ˜³', \"'\", 'Ãª', 'Ã©', 'ğŸ˜ˆ', 'Ã±', 'â– ', 'Û£', 'Â«', 'ğŸŒ¬', 'ÙŒ', 'ï»‘', 'ğŸ˜›', 'ïºª', 'ğŸ‘Œ', 'ğŸ™Œ', 'ğŸ’•', 'Ú„', 'ğŸ’š', 'Ø­', 'ğŸ—¯', 'â›”', 'ğŸ™‚', '@', 'ÙŠ', 'âœ', 'ï»°', 'ğŸ›¢', '-', 'ğŸ¬', 'ğŸ˜‡', 'âšª', 'â™¨', 'ï»®', 'ğŸ˜Ÿ', 'ğŸ˜¿', 'ğŸ€', 'ğŸ™…', 'T', 'ğŸ˜“', 'V', 'ğŸ’‰', 'ã€‹', '_', 'Û…', 'Ù‘', 'ğŸ‘', 'ğŸ‘‡', 'â€˜', 'ïº˜', 'ğŸ¼', 'ğŸ’Œ', 'âšœ', 'ïº‘', 'â™£', 'ã€Š', 'Â£', 'â–ª', '\"', 'B', 'Ù ', 'Ùª', 'k', 'ï»', 'â—', 'Ù', 'ï»µ', 'ğŸ‰', 'Ú¤', 'â€', 'ğŸ“', 'Ã¶', 'ïº•', 'K', 'âš¡', 'ï»Ÿ', 'ğŸ•¯', 'ğŸ¤¤', 'ï»›', 'ï»©', 'h', 'Ù¡', 'â±', 'ïº„', 'ğŸŒ¹', 'o', '3', '\\u202b', 'ï»£', 'ïº', 'v', 'ï»“', 'â™¥', 'Ã¯', 'ğŸŒ³', 'ğŸ’', 'â‡', 'ğŸ¥•', 'Ù‰', 'ğŸ’‘', '2', 'Øª', 'ğŸš«', 'Ø©', 'ğŸ§¿', 'âœ¨', 'â¤', 't', 'Ø¸', 'ğŸ’€', 'ï»¥', ' ', 'Â¤', 'a', '>', 'ğŸ˜', 'X', 'â€œ', 'Ø®', '&', '\\u061c', 'ïº ', 'â³', 'â˜ª', 'Ùˆ', '5', '\\u200d', 'ğŸ˜¥', 'ğŸ’“', 'ïº', 'Ø´', 'ï»¢', 'ğŸŒ§', 'R', 'Ú¨', 'Ã¤', 'b', 'ï´¾', 'Ã³', 'Ù‡', 'ğŸ’', 'Ù¨', 'ğŸ˜£', 'ğŸ¤«', 'Ù•', 'ğŸŒ·', 'â˜‚', 'ï»‹', 'âœ³', 'Ø¬', 'ğŸ‡¸', ':', 'ğŸŒ€', '<', 'İ£', 'ğŸŒ¸', 'ğŸ˜±', 'z', '6', 'Û—', 'ïº€', 'Ûš', 'ï´¿', 'ğŸ', 'i', ']', 'c', 'â–¶', 'ğŸ˜¯', 'ğŸ”', 'ğŸ˜œ', 'â˜¹', 'ïº®', 'Ú¥', 'A', 'ïºº', 'Ã', 'ğŸ˜®', 'ğŸ¤¨', 'ğŸ¤—', 'Ø¦', 'ï»˜', 'â£', 'Ã­', 'ğŸ”¶', 'ğŸ˜’', 'â—‡', 'ğŸ˜ª', 'W', 'â©', 'ïº©', '^', 'ğŸ‘‰', 'â˜€', 'ã€½', 'ï»š', 'Ã¡', 'ï» ', 'ï»ˆ', 'Ø¯', 'q', 'ïºŸ', 'Ù', 'e', '4', ';', 'â™¡', 'ÄŸ', 'ï»»', 'Ã§', 'ğŸ˜¡', 'u', 'ğŸ¤£', 'ïº’', 'ğŸ••', 'Ø·', 'ğŸ˜', 'ïº¢', 'ğŸ—¨', 'ğŸš', '+', 'L', 'Ù§', 'â„', 'ğŸ’Ÿ', 'M', 'Ú‰', 'Ù€', 'â˜º', 'ğŸ’', 'ğŸ˜·', 'ï»”', '/', 'Ø¡', 'S', 'Ù¾', '~', 'ğŸŒ¤', 'ïº­', 'ğŸ˜§']\n",
      "538\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ø±', 'Ø¤', 'ÙŠ', 'Ø§', ' ', 'Ù…', 'Ø³', 'Ø­', 'Øª', ' ', 'Ø¹', 'ÙŠ', 'Ù†', 'ÙŠ', 'Ù‡', 'Ø§', ' ', 'Ùˆ', 'Ø¹', 'Ù„', 'Ø§', 'Øª', ' ', 'Ø¹', 'ÙŠ', 'Ù†', 'ÙŠ', 'Ù‡', 'Ø§', ' ', 'Ù', 'ÙŠ', 'Ù‡', ' ', 'Ùˆ', 'Ùƒ', 'Ù„', 'Ø´', 'ÙŠ', ' ', 'Ù„', 'ÙŠ', ' ', 'Ùƒ', 'Ø§', 'Ù†', 'Ùˆ', ' ', 'Ø­', 'Ø§', 'Ø·', 'ÙŠ', 'Ù†', ' ', 'Ù„', 'Ù„', 'Ø¹', 'ÙŠ', 'Ù†', ' ', 'Ø¹', 'Ù„', 'ÙŠ', 'Ù‡', 'Ø§', '.', '.', '.', 'Ø´', 'Ø­', 'Ø§', 'Ù„', ' ', 'Ø¨', 'Ø§', 'Ø´', ' ', 'Ø¹', 'Ø§', 'Ø¯', ' ', 'Ù‡', 'Ø¶', 'Ø±', 'Øª']]\n"
     ]
    }
   ],
   "source": [
    "test_text = ['Ø±Ø¤ÙŠØ§ Ù…Ø³Ø­Øª Ø¹ÙŠÙ†ÙŠÙ‡Ø§ ÙˆØ¹Ù„Ø§Øª Ø¹ÙŠÙ†ÙŠÙ‡Ø§ ÙÙŠÙ‡ ÙˆÙƒÙ„Ø´ÙŠ Ù„ÙŠ ÙƒØ§Ù†Ùˆ Ø­Ø§Ø·ÙŠÙ† Ù„Ù„Ø¹ÙŠÙ† Ø¹Ù„ÙŠÙ‡Ø§...Ø´Ø­Ø§Ù„ Ø¨Ø§Ø´ Ø¹Ø§Ø¯ Ù‡Ø¶Ø±Øª']\n",
    "data = [[*x] for x in test_text]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-('ÙŠ', 'Ù†')\n",
      "1-(' ', 'Ø¹')\n",
      "2-('Ù‡', 'Ø§')\n",
      "3-('Øª', ' Ø¹')\n",
      "4-('Øª Ø¹', 'ÙŠÙ†')\n",
      "5-('.', '.')\n",
      "6-('Ø±', 'Ø¤')\n",
      "7-('Ù…', 'Ø³')\n",
      "8-('Ù„', 'Ù„')\n",
      "9-('Ø­', 'Øª Ø¹ÙŠÙ†')\n",
      "10-('Ùˆ', 'Ùƒ')\n",
      "11-('Ù‡Ø§', '..')\n",
      "12-('Ù…Ø³', 'Ø­Øª Ø¹ÙŠÙ†')\n",
      "13-"
     ]
    }
   ],
   "source": [
    "tokenized_data, vocab_bpe = bpe(data, vocab.copy(), k=30, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'ïº”', 'ïº·', 'Ã‡', 'ï»¦', '7', 'ğŸ‘Š', 'ğŸ‘‘', 'ğŸ˜”', 'ğŸ”ª', 'ğŸ»', 'Ù©', 'ğŸ’ª', 'â€¦', 'â›…', 'ğŸŒ¾', 'J', 'âœ‹', 'Ã¼', 'ğŸŒ¨', 'ğŸ’–', 'ïº¤', 'Ù¥', 'â™¬', 'ğŸŒ“', 'ğŸŒ²', '(', 'ğŸ˜²', 'â€¢', 'â€™', 'ï»·', 'ğŸ˜‹', 'ğŸ‘', 'ğŸ˜™', 'ğŸ’ ', 'ğŸ¤­', 'Ù', 'âˆš', 'f', 'ïº´', 'â—‹', 'ğŸ™†', 'ğŸ˜¶', 'ğŸ˜©', 'n', 'ï»™', 'ğŸ™„', 'ğŸ˜¢', 'ğŸš’', 'ğŸ’¦', 'â™¤', 'ğŸ‘„', '8', 'ğŸ“', 'ğŸ’¬', 'r', 'ğŸ’˜', 'ğŸ˜´', 'E', 'Ø°', 'âœ…', 'ğŸ›¡', 'ğŸ”š', '\\u2069', 'â˜†', ')', 'ğŸ’', 'ğŸƒ', 'ğŸ˜…', 'ÛŒ', 'ğŸƒ', 'Ù”', 'ğŸ‘ˆ', 'ï»³', 'ğŸ¥º', 'ğŸ«', 'ğŸ˜ ', 'Ã¨', 'Â°', 'â€º', 'N', 'ï¸', 'O', 'Ú©', 'Ú­', 'ğŸ“§', 'ğŸªµ', 'ğŸ”±', '9', 'âœ´', 'm', ',', 'ğŸ“±', 'ğŸ“', 'I', 'ğŸ–¤', 'P', 'ğŸ’«', 'ğŸ‡¦', 'Ù“', 'ğŸ˜µ', 'ã…¤', '%', 'ïº¸', 'ğŸ˜‚', 'ğŸ˜ƒ', 'âœŠ', 'U', 'j', 'ğŸ’ƒ', 'â¬‡', 'Ø£', 'ØŒ', 'â­', 'w', 'ïº£', 'ğŸŒ¿', 'ï»¤', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ—¡', 'ğŸš‘', 'ğŸ˜', 'ğŸ’œ', 'ğŸš¨', 'ğŸ¤', 'Ø¥', 'Ù', 'ïº§', 'Ã–', 'ğŸ‡²', 'y', 'Ø²', 'd', 'Ä±', 's', 'ğŸ”œ', 'Ù±', 'ğŸŒˆ', 'x', '\\\\', '=', 'ğŸŒŒ', 'ğŸ‘¨', 'ïº¡', 'F', 'ÅŸ', 'ğŸ˜•', 'Z', 'Ø›', '\\xad', '!', 'ğŸ’¯', '}', 'ğŸ¶', 'Q', 'âŠ™', 'ğŸ‘¶', 'D', 'ğŸ†', 'ï»', 'ğŸ˜Œ', '1', 'â€”', 'Ã²', '#', 'Ù¤', 'ï»¹', 'Ú†', 'ğŸ’£', 'Ãº', 'Ø¢', '{', 'ğŸ’', 'ğŸŒš', 'ğŸ’”', 'ğŸ˜¤', 'Øµ', 'ğŸ˜¦', 'â€¼', 'ğŸ˜', 'Øº', '|', 'ğŸ•³', 'ğŸ˜š', 'ğŸ‡º', 'ğŸ˜°', 'ğŸ˜¹', 'ïº»', 'Â¿', 'Ã®', 'ğŸ¸', 'Ù’', 'ï»Œ', 'Â¥', 'Â»', 'Ú', 'ğŸ’¢', 'Ù', 'ğŸ”®', 'ğŸŒŸ', 'G', 'ï»²', 'Ù£', 'ğŸ•—', 'ğŸ’§', 'Y', 'ï»¬', 'ğŸµ', 'ï»¨', 'C', 'ğŸˆ', 'â–½', 'ğŸ˜—', 'Ù‚', 'ï»§', 'ğŸŒº', 'â€¹', 'â™¦', 'ğŸ”°', '\\u2066', 'ğŸ™Š', 'ğŸ‘®', 'ïº³', 'ğŸŒ', 'ğŸ¤”', 'Ú¯', '*', 'Ø¶', 'ğŸ”¥', 'Ù¢', 'ğŸ˜­', 'ğŸ', 'ğŸ“ƒ', 'Ù°', 'ğŸ˜¨', 'ØŸ', 'ğŸ‚', 'ïº', 'Ø¨', 'â€', '\\u202c', 'p', 'ğŸ‘', 'ğŸš—', 'ğŸ˜', 'ğŸ‘', '0', 'ï»´', 'ïº¼', 'ïºƒ', 'ğŸ‘‚', 'ğŸ˜‰', 'â™€', 'â™§', 'ğŸ˜Š', 'ğŸ¤•', '?', 'ï»€', 'ğŸ˜‘', 'ï»­', 'l', 'ğŸ²', 'ğŸŒ—', 'ğŸ‘º', 'ğŸŒ', 'Û', 'g', 'â—', 'â†’', 'ğŸ™€', 'Ø«', 'Ù¦', 'ğŸ’¨', 'â–¼', 'ğŸ”ˆ', 'ğŸ’‹', 'ğŸ”™', 'ğŸ’¥', 'ïº–', 'ğŸ˜˜', 'Ã—', '[', 'ğŸ””', 'Ù‹', 'âŒ', 'Ã¢', 'â˜', 'ğŸ˜³', \"'\", 'Ãª', 'Ã©', 'ğŸ˜ˆ', 'Ã±', 'â– ', 'Û£', 'Â«', 'ğŸŒ¬', 'ÙŒ', 'ï»‘', 'ğŸ˜›', 'ïºª', 'ğŸ‘Œ', 'ğŸ™Œ', 'ğŸ’•', 'Ú„', 'ğŸ’š', 'ğŸ—¯', 'â›”', 'ğŸ™‚', '@', 'âœ', 'ï»°', 'ğŸ›¢', '-', 'ğŸ¬', 'ğŸ˜‡', 'âšª', 'â™¨', 'ï»®', 'ğŸ˜Ÿ', 'ğŸ˜¿', 'ğŸ€', 'ğŸ™…', 'T', 'ğŸ˜“', 'V', 'ğŸ’‰', 'ã€‹', '_', 'Û…', 'Ù‘', 'ğŸ‘', 'ğŸ‘‡', 'â€˜', 'ïº˜', 'ğŸ¼', 'ğŸ’Œ', 'âšœ', 'ïº‘', 'â™£', 'ã€Š', 'Â£', 'â–ª', '\"', 'B', 'Ù ', 'Ùª', 'k', 'ï»', 'â—', 'Ù', 'ï»µ', 'ğŸ‰', 'Ú¤', 'â€', 'ğŸ“', 'Ã¶', 'ïº•', 'K', 'âš¡', 'ï»Ÿ', 'ğŸ•¯', 'ğŸ¤¤', 'ï»›', 'ï»©', 'h', 'Ù¡', 'â±', 'ïº„', 'ğŸŒ¹', 'o', '3', '\\u202b', 'ï»£', 'ïº', 'v', 'ï»“', 'â™¥', 'Ã¯', 'ğŸŒ³', 'ğŸ’', 'â‡', 'ğŸ¥•', 'Ù‰', 'ğŸ’‘', '2', 'ğŸš«', 'Ø©', 'ğŸ§¿', 'âœ¨', 'â¤', 't', 'Ø¸', 'ğŸ’€', 'ï»¥', 'Â¤', 'a', '>', 'ğŸ˜', 'X', 'â€œ', 'Ø®', '&', '\\u061c', 'ïº ', 'â³', 'â˜ª', '5', '\\u200d', 'ğŸ˜¥', 'ğŸ’“', 'ïº', 'Ø´', 'ï»¢', 'ğŸŒ§', 'R', 'Ú¨', 'Ã¤', 'b', 'ï´¾', 'Ã³', 'ğŸ’', 'Ù¨', 'ğŸ˜£', 'ğŸ¤«', 'Ù•', 'ğŸŒ·', 'â˜‚', 'ï»‹', 'âœ³', 'Ø¬', 'ğŸ‡¸', ':', 'ğŸŒ€', '<', 'İ£', 'ğŸŒ¸', 'ğŸ˜±', 'z', '6', 'Û—', 'ïº€', 'Ûš', 'ï´¿', 'ğŸ', 'i', ']', 'c', 'â–¶', 'ğŸ˜¯', 'ğŸ”', 'ğŸ˜œ', 'â˜¹', 'ïº®', 'Ú¥', 'A', 'ïºº', 'Ã', 'ğŸ˜®', 'ğŸ¤¨', 'ğŸ¤—', 'Ø¦', 'ï»˜', 'â£', 'Ã­', 'ğŸ”¶', 'ğŸ˜’', 'â—‡', 'ğŸ˜ª', 'W', 'â©', 'ïº©', '^', 'ğŸ‘‰', 'â˜€', 'ã€½', 'ï»š', 'Ã¡', 'ï» ', 'ï»ˆ', 'Ø¯', 'q', 'ïºŸ', 'Ù', 'e', '4', ';', 'â™¡', 'ÄŸ', 'ï»»', 'Ã§', 'ğŸ˜¡', 'u', 'ğŸ¤£', 'ïº’', 'ğŸ••', 'Ø·', 'ğŸ˜', 'ïº¢', 'ğŸ—¨', 'ğŸš', '+', 'L', 'Ù§', 'â„', 'ğŸ’Ÿ', 'M', 'Ú‰', 'Ù€', 'â˜º', 'ğŸ’', 'ğŸ˜·', 'ï»”', '/', 'Ø¡', 'S', 'Ù¾', '~', 'ğŸŒ¤', 'ïº­', 'ğŸ˜§', 'Ø±Ø¤', 'Ù„Ù„', 'ÙˆÙƒ', 'Ù‡Ø§..', 'Ù…Ø³Ø­Øª Ø¹ÙŠÙ†']\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "print(vocab_bpe)\n",
    "print(len(vocab_bpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ø±', 'Ø¤', 'ÙŠ', 'Ø§', ' ', 'Ù…', 'Ø³', 'Ø­', 'Øª', ' ', 'Ø¹', 'ÙŠ', 'Ù†', 'ÙŠ', 'Ù‡', 'Ø§', ' ', 'Ùˆ', 'Ø¹', 'Ù„', 'Ø§', 'Øª', ' ', 'Ø¹', 'ÙŠ', 'Ù†', 'ÙŠ', 'Ù‡', 'Ø§', ' ', 'Ù', 'ÙŠ', 'Ù‡', ' ', 'Ùˆ', 'Ùƒ', 'Ù„', 'Ø´', 'ÙŠ', ' ', 'Ù„', 'ÙŠ', ' ', 'Ùƒ', 'Ø§', 'Ù†', 'Ùˆ', ' ', 'Ø­', 'Ø§', 'Ø·', 'ÙŠ', 'Ù†', ' ', 'Ù„', 'Ù„', 'Ø¹', 'ÙŠ', 'Ù†', ' ', 'Ø¹', 'Ù„', 'ÙŠ', 'Ù‡', 'Ø§', '.', '.', '.', 'Ø´', 'Ø­', 'Ø§', 'Ù„', ' ', 'Ø¨', 'Ø§', 'Ø´', ' ', 'Ø¹', 'Ø§', 'Ø¯', ' ', 'Ù‡', 'Ø¶', 'Ø±', 'Øª']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokenize_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_bpe\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtokenize_bpe\u001b[1;34m(data, vocab)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m sent[x: x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tkn)]\n\u001b[1;32m---> 11\u001b[0m             \u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtkn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenize_bpe(tokenized_data, vocab_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¶\n"
     ]
    }
   ],
   "source": [
    "print(vocab_bpe[228])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
